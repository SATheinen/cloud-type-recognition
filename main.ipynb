{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e0472-1f98-4a5b-950b-5ab84f66411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_cases = False # Set to True if debugging is required\n",
    "cloud_labels = [\"Flower\", \"Gravel\", \"Fish\", \"Sugar\"] # All possible labels for the clouds\n",
    "\n",
    "# Original Image resolutions\n",
    "in_res_y = 1400\n",
    "in_res_x = 2100\n",
    "\n",
    "# New Image resolutions\n",
    "new_res_y = 512\n",
    "new_res_x = 768\n",
    "\n",
    "# data directories\n",
    "test_dir = \"./test_images\"\n",
    "train_dir = \"./train_images\"\n",
    "\n",
    "# Training params\n",
    "num_filters = 24 # Number of filters in first conv layer  \n",
    "num_train_images = 128\n",
    "num_test_images = 32\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 40\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78d74dc-82d1-40ff-a5b5-a9369c14a606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['af4d36b.jpg', 'ebe596d.jpg', '423c22d.jpg', '61259d9.jpg', 'bacd6fe.jpg', '1230c60.jpg', '4b8a262.jpg', '3614409.jpg', '3361513.jpg', '7473a71.jpg', 'f431307.jpg', '0e42958.jpg', '4e5b275.jpg', 'a1d13fa.jpg', '57559fb.jpg', 'df426c2.jpg', '6e1e7cb.jpg', 'a00ab01.jpg', '5e70a5b.jpg', '6d9de9e.jpg', 'd836ac5.jpg', '19807b1.jpg', '5683db4.jpg', '40dd239.jpg', '87a1831.jpg', 'fa12d07.jpg', '741dcf3.jpg', 'e3b3009.jpg', '2d2bd73.jpg', 'd2eb9fe.jpg', 'de2c9bb.jpg', 'cea2726.jpg']\n"
     ]
    }
   ],
   "source": [
    "image_names = os.listdir(train_dir)\n",
    "\n",
    "train_images = image_names[:num_train_images]\n",
    "test_images = image_names[num_train_images:num_train_images+num_test_images]\n",
    "print(test_images)\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df[['Image', 'Label']] = df['Image_Label'].str.split('_', expand=True)\n",
    "\n",
    "if run_test_cases:\n",
    "    print(df[['Image', 'Label', 'EncodedPixels']].head(8))\n",
    "    print()\n",
    "    print(df['Image'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4364d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels and rle from image name\n",
    "def get_labels_rle(image_name: str, df) -> list:\n",
    "    rles = df[df['Image'] == image_name]['EncodedPixels'].to_list()\n",
    "    labels = df[df['Image'] == image_name]['Label'].to_list()\n",
    "    return rles, labels\n",
    "\n",
    "# Debugging\n",
    "if run_test_cases:\n",
    "\n",
    "    # Get Files\n",
    "    train_images = os.listdir(train_dir)[:2]\n",
    "    print(f\"Train images: {train_images}\")\n",
    "\n",
    "    for image in train_images:\n",
    "        rles, labels = get_labels_rle(f\"{image}\", df)\n",
    "        for rle, label in zip(rles, labels):\n",
    "            print(f\"Label: {label} \\n rle: {rle} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095e0668-beaa-4f16-86c3-2d357ca784a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rle mask encoding into 2D arrays\n",
    "def rle_to_array(rle_list: list) -> np.array:\n",
    "\n",
    "    # Create empty array for\n",
    "    array = np.zeros(in_res_y * in_res_x)\n",
    "\n",
    "    # Skip if cloud formation is not on picture\n",
    "    if not rle_list or pd.isna(rle_list):\n",
    "        mask = array.reshape((in_res_x, in_res_y), order=\"F\").T\n",
    "        return mask\n",
    "    \n",
    "    rle_array = np.array(list(map(int, rle_list.split())), dtype=int)\n",
    "    start_pixels = rle_array[::2] - 1 # Offset because pixel 1 is arr position 0\n",
    "    num_pixels = rle_array[1::2]\n",
    "\n",
    "    # Create 2D mask\n",
    "    for start_pixel, num_pixels in zip(start_pixels, num_pixels): # Format is [start_idx_0, num_pixels_0 ...]\n",
    "        array[start_pixel:start_pixel+num_pixels] = 1.0\n",
    "    \n",
    "    # Reshape\n",
    "    mask = array.reshape((in_res_x, in_res_y), order=\"A\").T # 2D array of [Height, Width]\n",
    "\n",
    "    return mask\n",
    "\n",
    "# For debugging\n",
    "if run_test_cases:\n",
    "\n",
    "    # Get Files\n",
    "    train_images = os.listdir(train_dir)[:2]\n",
    "    print(f\"Train images: {train_images}\")\n",
    "\n",
    "    # Plot files\n",
    "    for image_name in train_images:\n",
    "        img = cv2.imread(f\"{train_dir}/{image_name}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
    "\n",
    "        rles, labels = get_labels_rle(image_name, df)\n",
    "\n",
    "        # Raw Image\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        for rle, label in zip(rles, labels):\n",
    "\n",
    "            # Masked Image\n",
    "            mask = rle_to_array(rle)\n",
    "            print(np.unique(mask))\n",
    "            print(f\"titel: {label}\")\n",
    "            plt.imshow(mask, cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "            #plt.imshow(mask[:, :, None].repeat(3, axis=-1)*img, cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "            plt.show()\n",
    "        print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b649f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(preds, target, eps=1e-6):\n",
    "    # [B, 4, H, W]\n",
    "\n",
    "    preds = torch.sigmoid(preds)\n",
    "    overlap = (preds * target).sum((1,2,3))\n",
    "\n",
    "    dice = (2. * overlap + eps) / (preds.sum((1,2,3)) + target.sum((1,2,3)) + eps)\n",
    "\n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(preds, target, eps=1e-6):\n",
    "    return 1 - dice_coef(preds, target, eps=eps)\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206aadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, target, eps=1e-6):\n",
    "    return 0.5 * dice_loss(preds, target, eps) + 0.5 * bce_loss(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c257c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images: [3, H, W]\n",
    "# Masks : [4, H, W] (4 cloud types)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_frame, img_dir, image_names):\n",
    "        self.img_dir = img_dir\n",
    "        self.image_names = image_names\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        image_name = self.image_names[idx]\n",
    "\n",
    "        # Read in image files\n",
    "        image = cv2.imread(f\"{self.img_dir}/{image_name}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # Downsample\n",
    "        image = cv2.resize(image, (new_res_x, new_res_y), interpolation=cv2.INTER_AREA)\n",
    "        image = (image / 255.0).astype(np.float32)\n",
    "        image = torch.from_numpy(image).float()\n",
    "\n",
    "        rles, _ = get_labels_rle(image_name, self.data_frame)\n",
    "\n",
    "        # Create mask\n",
    "        mask = torch.zeros((4, in_res_y, in_res_x), dtype=torch.float32) # [4, H, W]\n",
    "\n",
    "        for i, rle in enumerate(rles):\n",
    "            single_mask = rle_to_array(rle)\n",
    "            single_mask = torch.from_numpy(single_mask).float()\n",
    "            mask[i, :, :] = single_mask\n",
    "\n",
    "        # Downsample mask\n",
    "        mask = mask.unsqueeze(0)\n",
    "        mask = F.interpolate(mask.float(), size=(new_res_y, new_res_x), mode='nearest').long()\n",
    "        mask = mask.squeeze(0)\n",
    "\n",
    "        # [H, W, C] -> [C, H, W]\n",
    "        image = image.transpose(0, 2).transpose(1, 2) # [C, H, W]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13727c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size, padding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, conv_kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, conv_kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv_block(x)\n",
    "        return output # [batch, out_channels, H_out, W_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b7b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolBlock(nn.Module):\n",
    "    def __init__(self, downsample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(downsample)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pool(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4cf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, channels, upsample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up_sample_block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=upsample, mode='bilinear', align_corners=False)\n",
    "            #nn.Conv2d(channels, channels // upsample, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        upsample_block = self.up_sample_block(x)\n",
    "        return upsample_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5351dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "\n",
    "        # in_channels, out_channels, conv_kernel_size, padding, pool_kernel_size\n",
    "        self.encoder_layer_0 = ConvolutionBlock(3, filters, 3, 1)\n",
    "        self.encoder_layer_1 = ConvolutionBlock(filters, 2*filters, 3, 1)\n",
    "\n",
    "        self.pool_block_0 = PoolBlock(downsample=2)\n",
    "        self.pool_block_1 = PoolBlock(downsample=2)\n",
    "\n",
    "        self.bottle_neck = nn.Conv2d(2*filters, 4*filters, 3, padding=1)\n",
    "\n",
    "        # in_channels, out_channels, conv_kernel_size, stride\n",
    "        self.decoder_layer_1 = ConvolutionBlock(4*filters, 2*filters, 3, 1)\n",
    "        self.decoder_layer_0 = ConvolutionBlock(2*filters, filters, 3, 1)\n",
    "\n",
    "        self.up_sample_block1 = UpSampleBlock(4 * filters, upsample=2)\n",
    "        self.up_sample_block0 = UpSampleBlock(2 * filters, upsample=2)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(filters, 4, 1)\n",
    "\n",
    "    def forward(self, x): # [Batch, Color, Height, Width]\n",
    "        \n",
    "        enc0 = self.encoder_layer_0(x) # [B, num_filters, H, W]\n",
    "        pool0 = self.pool_block_0(enc0) # [B, num_filters, H / 2, W / 2]\n",
    "\n",
    "        enc1 = self.encoder_layer_1(pool0) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "        pool1 = self.pool_block_1(enc1) # [B, 2 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        bottle_neck = self.bottle_neck(pool1) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        up1 = self.up_sample_block1(bottle_neck) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "        #concat1 = torch.cat([up1, enc1], dim=1) # [B, 4 * num_filters, H / 2, W / 2]\n",
    "        dec1 = self.decoder_layer_1(up1) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "\n",
    "        up0 = self.up_sample_block0(dec1) # [B, num_filters, H, W]\n",
    "        #concat0 = torch.cat([up0, enc0], dim=1) # [B, 2 * num_filters, H, W]\n",
    "        dec0 = self.decoder_layer_0(up0) # [B, num_filters, H, W]\n",
    "\n",
    "        logits = self.output_layer(dec0) # [B, 4, H, W]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef01b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and DataLoader\n",
    "train_dataset = ImageDataset(df, train_dir, train_images)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=8)\n",
    "\n",
    "test_dataset = ImageDataset(df, train_dir, test_images)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(num_filters).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49169cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 0.7668\n",
      "Val loss: 0.7789\n",
      "Dice coefficient: 0.2176\n",
      "\n",
      "Epoch: 1\n",
      "Train loss: 0.7537\n",
      "Val loss: 0.7787\n",
      "Dice coefficient: 0.2234\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 0.7500\n",
      "Val loss: 0.7606\n",
      "Dice coefficient: 0.2284\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 17\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for image, mask in train_dataloader:\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    dice = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, mask in test_dataloader:\n",
    "            image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, mask)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            hard_preds = torch.where(preds > 0.5, 1.0, 0.0)\n",
    "            dice += dice_coef(hard_preds, mask)\n",
    "        val_loss /= len(test_dataloader)\n",
    "        dice /= len(test_dataloader)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Val loss: {val_loss:.4f}\")\n",
    "    print(f\"Dice coefficient: {dice:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nest_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

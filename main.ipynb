{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2e0472-1f98-4a5b-950b-5ab84f66411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37bbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_cases = False # Set to True if debugging is required\n",
    "cloud_labels = [\"Flower\", \"Gravel\", \"Fish\", \"Sugar\"] # All possible labels for the clouds\n",
    "\n",
    "# Original Image resolutions\n",
    "in_res_y = 1400\n",
    "in_res_x = 2100\n",
    "\n",
    "# New Image resolutions\n",
    "tile_size = (512, 512)\n",
    "stride = (512, 512)\n",
    "\n",
    "# data directories\n",
    "test_dir = \"./test_images\"\n",
    "train_dir = \"./train_images\"\n",
    "\n",
    "# Model specifications\n",
    "model_name = \"pretrained\" # [custom, pretrained]\n",
    "num_filters = 32 # Number of filters in first conv layer (only for custom network)\n",
    "\n",
    "# Training params\n",
    "num_train_images = 5040\n",
    "num_test_images = 256\n",
    "\n",
    "num_workers = 0 # Number of workers for Dataset creation\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "lr = 6e-5\n",
    "weight_decay = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78d74dc-82d1-40ff-a5b5-a9369c14a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe\n",
    "df = pd.read_csv('train.csv')\n",
    "df[['Image', 'Label']] = df['Image_Label'].str.split('_', expand=True)\n",
    "\n",
    "# Find images with at least one mask\n",
    "df_nonempty = df.groupby(\"Image\")[\"EncodedPixels\"].apply(lambda x: x.notna().any()).reset_index()\n",
    "df_nonempty = df_nonempty[df_nonempty[\"EncodedPixels\"] == True]\n",
    "valid_images = df_nonempty[\"Image\"].unique().tolist()\n",
    "\n",
    "image_names = [img for img in sorted(os.listdir(train_dir)) if img in valid_images]\n",
    "\n",
    "train_images = image_names[:num_train_images]\n",
    "test_images = image_names[num_train_images:num_train_images+num_test_images]\n",
    "\n",
    "if run_test_cases:\n",
    "    print(df[['Image', 'Label', 'EncodedPixels']].head(8))\n",
    "    print()\n",
    "    print(df['Image'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4364d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels and rle from image name\n",
    "def get_labels_rle(image_name: str, df) -> list:\n",
    "    rles = df[df['Image'] == image_name]['EncodedPixels'].to_list()\n",
    "    labels = df[df['Image'] == image_name]['Label'].to_list()\n",
    "    return rles, labels\n",
    "\n",
    "# Debugging\n",
    "if run_test_cases:\n",
    "\n",
    "    # Get Files\n",
    "    test_train_images = os.listdir(train_dir)[:4]\n",
    "    print(f\"Train images: {test_train_images}\")\n",
    "\n",
    "    for image in test_train_images:\n",
    "        rles, labels = get_labels_rle(f\"{image}\", df)\n",
    "        for rle, label in zip(rles, labels):\n",
    "            print(f\"Label: {label} \\n rle: {rle} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095e0668-beaa-4f16-86c3-2d357ca784a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rle mask encoding into 2D arrays\n",
    "def rle_to_array(rle_list: list) -> np.array:\n",
    "\n",
    "    # Create empty array for\n",
    "    array = np.zeros(in_res_y * in_res_x)\n",
    "\n",
    "    # Skip if cloud formation is not on picture\n",
    "    if not rle_list or pd.isna(rle_list):\n",
    "        mask = array.reshape((in_res_x, in_res_y), order=\"A\").T\n",
    "        return mask\n",
    "    \n",
    "    rle_array = np.array(list(map(int, rle_list.split())), dtype=int)\n",
    "    start_pixels = rle_array[::2] - 1 # Offset because pixel 1 is arr position 0\n",
    "    num_pixels = rle_array[1::2]\n",
    "\n",
    "    # Create 2D mask\n",
    "    for start_pixel, num_pixels in zip(start_pixels, num_pixels): # Format is [start_idx_0, num_pixels_0 ...]\n",
    "        array[start_pixel:start_pixel+num_pixels] = 1.0\n",
    "    \n",
    "    # Reshape\n",
    "    mask = array.reshape((in_res_x, in_res_y), order=\"A\").T # 2D array of [Height, Width]\n",
    "\n",
    "    return mask\n",
    "\n",
    "# For debugging\n",
    "if run_test_cases:\n",
    "\n",
    "    # Get Files\n",
    "    test_train_images = os.listdir(train_dir)[:2]\n",
    "    print(f\"Train images: {test_train_images}\")\n",
    "\n",
    "    # Plot files\n",
    "    for image_name in test_train_images:\n",
    "        img = cv2.imread(f\"{train_dir}/{image_name}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        rles, labels = get_labels_rle(image_name, df)\n",
    "\n",
    "        # Raw Image\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        for rle, label in zip(rles, labels):\n",
    "\n",
    "            # Masked Image\n",
    "            mask = rle_to_array(rle)\n",
    "            print(np.unique(mask))\n",
    "            print(f\"titel: {label}\")\n",
    "            plt.imshow(mask, cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "            #plt.imshow(mask[:, :, None].repeat(3, axis=-1)*img, cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "            plt.show()\n",
    "        print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b649f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(preds, target, eps=1e-6):\n",
    "    # [B, 4, H, W]\n",
    "\n",
    "    preds = torch.sigmoid(preds)\n",
    "    overlap = (preds * target).sum((2,3))\n",
    "\n",
    "    dice = (2. * overlap + eps) / (preds.sum((2,3)) + target.sum((2,3)) + eps)\n",
    "\n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e780c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(preds, target, eps=1e-6):\n",
    "    preds = preds.float()\n",
    "    target = target.float()\n",
    "    return 1 - dice_coef(preds, target, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4052a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight=torch.tensor([4.84217556, 5.62148491, 4.92171107, 4.60515229], dtype=torch.float32, device=device)\n",
    "\n",
    "def bce_loss(logits, targets, pos_weight=pos_weight):\n",
    "    \"\"\"\n",
    "    Custom BCE with per-class pos_weight pre-applied.\n",
    "    logits: [B, C, H, W]\n",
    "    targets: [B, C, H, W]\n",
    "    pos_weight: [C]\n",
    "    \"\"\"\n",
    "    logits = logits.float()\n",
    "    targets = targets.float()\n",
    "\n",
    "    # Broadcast pos_weight to (1, C, 1, 1)\n",
    "    pos_weight = pos_weight.view(1, -1, 1, 1)\n",
    "\n",
    "    # Compute sigmoid\n",
    "    pred = torch.sigmoid(logits)\n",
    "\n",
    "    # Compute standard BCE loss per element\n",
    "    loss = -(\n",
    "        pos_weight * targets * torch.log(pred + 1e-8) +\n",
    "        (1 - targets) * torch.log(1 - pred + 1e-8)\n",
    "    )\n",
    "\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "206aadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, target, split=0.5, eps=1e-6):\n",
    "    return split * dice_loss(preds, target, eps) + (1 - split) * bce_loss(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f993a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/lhm9rkfd5mq5nqs778m806v40000gn/T/ipykernel_80750/4052266391.py:20: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n",
      "  A.ElasticTransform(alpha=50, sigma=8, alpha_affine=8, p=0.7),\n",
      "/var/folders/31/lhm9rkfd5mq5nqs778m806v40000gn/T/ipykernel_80750/4052266391.py:22: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(5, 20), p=0.7),\n",
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/var/folders/31/lhm9rkfd5mq5nqs778m806v40000gn/T/ipykernel_80750/4052266391.py:34: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes, min_height, min_width, fill_value, mask_fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(\n"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose([\n",
    "    # --- geometric transforms ---\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "\n",
    "    # --- Random crop + resize ---\n",
    "    #A.RandomResizedCrop(\n",
    "    #    height=tile_size[0], width=tile_size[1],\n",
    "    #    scale=(0.7, 1.0),  # crop 70–100% of image\n",
    "    #    ratio=(0.9, 1.1),  # aspect ratio jitter\n",
    "    #    p=0.5\n",
    "    #),\n",
    "\n",
    "    # --- photometric transforms ---\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=5, val_shift_limit=5, p=0.5),\n",
    "    \n",
    "    A.ElasticTransform(alpha=50, sigma=8, alpha_affine=8, p=0.7),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.7),\n",
    "    A.GaussNoise(var_limit=(5, 20), p=0.7),\n",
    "\n",
    "    # --- spatial transforms ---\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.05,\n",
    "        scale_limit=0.1,\n",
    "        rotate_limit=15,\n",
    "        border_mode=cv2.BORDER_REFLECT,\n",
    "        p=0.5\n",
    "    ),\n",
    "\n",
    "    # --- Cutout / dropout ---\n",
    "    A.CoarseDropout(\n",
    "        max_holes=8, max_height=64, max_width=64,\n",
    "        min_holes=1, min_height=16, min_width=16,\n",
    "        fill_value=0, mask_fill_value=0, p=0.5\n",
    "    ),\n",
    "\n",
    "    # --- normalization ---\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    # --- ensure correct shape ---\n",
    "    A.Resize(height=tile_size[0], width=tile_size[1]),\n",
    "\n",
    "    # --- to tensor ---\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    # --- normalization ---\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    # --- ensure correct shape ---\n",
    "    A.Resize(height=tile_size[0], width=tile_size[1]),\n",
    "\n",
    "    # --- to tensor ---\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c257c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_frame,\n",
    "        img_dir,\n",
    "        image_names,\n",
    "        tile_size=(512, 512),\n",
    "        stride=(512, 512),\n",
    "        transform=None,\n",
    "        keep_empty_prob=0.1,  # 10% of empty tiles\n",
    "    ):\n",
    "        self.img_dir = img_dir\n",
    "        self.image_names = image_names\n",
    "        self.tile_size = tile_size\n",
    "        self.stride = stride\n",
    "        self.data_frame = data_frame\n",
    "        self.transform = transform\n",
    "        self.keep_empty_prob = keep_empty_prob\n",
    "        self.tiles_index = []\n",
    "\n",
    "        self._prepare_tile_indices()\n",
    "\n",
    "    def pad_to_tile_size(self, image, mask):\n",
    "        \"\"\"Pad image & mask so height/width become multiples of tile_size.\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        th, tw = self.tile_size\n",
    "        pad_h = (th - h % th) % th\n",
    "        pad_w = (tw - w % tw) % tw\n",
    "\n",
    "        image_padded = np.pad(\n",
    "            image, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\"\n",
    "        )\n",
    "        mask_padded = np.pad(\n",
    "            mask, ((0, 0), (0, pad_h), (0, pad_w)), mode=\"constant\"\n",
    "        )\n",
    "        return image_padded, mask_padded\n",
    "\n",
    "    def _prepare_tile_indices(self):\n",
    "        \"\"\"Precompute tile indices and filter out empty ones (keep 10% of them).\"\"\"\n",
    "        th, tw = self.tile_size\n",
    "        sh, sw = self.stride\n",
    "        h, w = in_res_y, in_res_x  # canonical image size (1400x2100)\n",
    "\n",
    "        y_positions = range(0, h + ((th - h % th) % th), sh)\n",
    "        x_positions = range(0, w + ((tw - w % tw) % tw), sw)\n",
    "\n",
    "        for img_idx, img_name in enumerate(self.image_names):\n",
    "            # --- Load full mask once ---\n",
    "            rles, labels = get_labels_rle(img_name, self.data_frame)\n",
    "            mask = np.zeros((len(cloud_labels), h, w), dtype=np.float32)\n",
    "            for label, rle in zip(labels, rles):\n",
    "                if rle is not None and label in cloud_labels:\n",
    "                    mask[cloud_labels.index(label)] = rle_to_array(rle)\n",
    "\n",
    "            # --- Pad to tile size ---\n",
    "            _, mask = self.pad_to_tile_size(np.zeros((h, w, 3)), mask)\n",
    "\n",
    "            # --- Iterate over tiles ---\n",
    "            for y, x in product(y_positions, x_positions):\n",
    "                tile = mask[:, y : y + th, x : x + tw]\n",
    "                if tile.sum() > 50:\n",
    "                    self.tiles_index.append((img_idx, y, x))\n",
    "                else:\n",
    "                    # keep only 10% of empty tiles\n",
    "                    if random.random() < self.keep_empty_prob:\n",
    "                        self.tiles_index.append((img_idx, y, x))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tiles_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx, y, x = self.tiles_index[idx]\n",
    "        image_name = self.image_names[img_idx]\n",
    "\n",
    "        # --- Load image ---\n",
    "        image = cv2.imread(f\"{self.img_dir}/{image_name}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "\n",
    "        # --- Load full mask ---\n",
    "        rles, labels = get_labels_rle(image_name, self.data_frame)\n",
    "        mask = np.zeros((len(cloud_labels), image.shape[0], image.shape[1]), dtype=np.float32)\n",
    "        for label, rle in zip(labels, rles):\n",
    "            if rle is not None and label in cloud_labels:\n",
    "                single_mask = rle_to_array(rle)\n",
    "                mask[cloud_labels.index(label)] = single_mask\n",
    "\n",
    "        # --- Pad too small images ---\n",
    "        image, mask = self.pad_to_tile_size(image, mask)\n",
    "\n",
    "        # --- Crop tile ---\n",
    "        th, tw = self.tile_size\n",
    "        image_tile = image[y : y + th, x : x + tw]\n",
    "        mask_tile = mask[:, y : y + th, x : x + tw]\n",
    "\n",
    "        image_tile = image_tile.astype(np.uint8)\n",
    "\n",
    "        # --- Transformations ---\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image_tile,\n",
    "                mask=mask_tile.transpose(1, 2, 0)\n",
    "            )\n",
    "            image_tile = transformed[\"image\"]\n",
    "            mask_tile = transformed[\"mask\"].permute(2, 0, 1)\n",
    "        else:\n",
    "            image_tile = torch.from_numpy(image_tile).permute(2, 0, 1).float()\n",
    "            mask_tile = torch.from_numpy(mask_tile).float()\n",
    "\n",
    "        return image_tile, mask_tile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13727c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size, padding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, conv_kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, conv_kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv_block(x)\n",
    "        return output # [batch, out_channels, H_out, W_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7b7b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolBlock(nn.Module):\n",
    "    def __init__(self, downsample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(downsample)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pool(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a4cf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, channels, upsample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up_sample_block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=upsample, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(channels, channels // upsample, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        upsample_block = self.up_sample_block(x)\n",
    "        return upsample_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5351dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "\n",
    "        # in_channels, out_channels, conv_kernel_size, padding, pool_kernel_size\n",
    "        self.encoder_layer_0 = ConvolutionBlock(3, filters, 3, 1)\n",
    "        self.encoder_layer_1 = ConvolutionBlock(filters, 2*filters, 3, 1)\n",
    "        self.encoder_layer_2 = ConvolutionBlock(2*filters, 4*filters, 3, 1)\n",
    "\n",
    "        self.pool_block_0 = PoolBlock(downsample=2)\n",
    "        self.pool_block_1 = PoolBlock(downsample=2)\n",
    "        self.pool_block_2 = PoolBlock(downsample=2)\n",
    "\n",
    "        self.bottle_neck = nn.Conv2d(4*filters, 8*filters, 3, padding=1)\n",
    "\n",
    "        # in_channels, out_channels, conv_kernel_size, stride\n",
    "        self.decoder_layer_2 = ConvolutionBlock(8*filters, 4*filters, 3, 1)\n",
    "        self.decoder_layer_1 = ConvolutionBlock(4*filters, 2*filters, 3, 1)\n",
    "        self.decoder_layer_0 = ConvolutionBlock(2*filters, filters, 3, 1)\n",
    "\n",
    "        self.up_sample_block2 = UpSampleBlock(8 * filters, upsample=2)\n",
    "        self.up_sample_block1 = UpSampleBlock(4 * filters, upsample=2)\n",
    "        self.up_sample_block0 = UpSampleBlock(2 * filters, upsample=2)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(filters, 4, 1)\n",
    "\n",
    "    def forward(self, x): # [Batch, Color, Height, Width]\n",
    "        \n",
    "        enc0 = self.encoder_layer_0(x) # [B, num_filters, H, W]\n",
    "        pool0 = self.pool_block_0(enc0) # [B, num_filters, H / 2, W / 2]\n",
    "\n",
    "        enc1 = self.encoder_layer_1(pool0) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "        pool1 = self.pool_block_1(enc1) # [B, 2 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        enc2 = self.encoder_layer_2(pool1) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "        pool2 = self.pool_block_2(enc2) # [B, 4 * num_filters, H / 8, W / 8]\n",
    "\n",
    "        bottle_neck = self.bottle_neck(pool2) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        up2 = self.up_sample_block2(bottle_neck) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "        concat2 = torch.cat([up2, enc2], dim=1) # [B, 8 * num_filters, H / 4, W / 4]\n",
    "        dec2 = self.decoder_layer_2(concat2) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        up1 = self.up_sample_block1(dec2) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "        concat1 = torch.cat([up1, enc1], dim=1) # [B, 4 * num_filters, H / 2, W / 2]\n",
    "        dec1 = self.decoder_layer_1(concat1) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "\n",
    "        up0 = self.up_sample_block0(dec1) # [B, num_filters, H, W]\n",
    "        concat0 = torch.cat([up0, enc0], dim=1) # [B, 2 * num_filters, H, W]\n",
    "        dec0 = self.decoder_layer_0(concat0) # [B, num_filters, H, W]\n",
    "\n",
    "        logits = self.output_layer(dec0) # [B, 4, H, W]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef01b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Datasets and DataLoader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m      4\u001b[0m                               pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#, persistent_workers = True, prefetch_factor = 2)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m ImageDataset(df, train_dir, test_images, tile_size\u001b[38;5;241m=\u001b[39mtile_size, stride\u001b[38;5;241m=\u001b[39mstride, transform\u001b[38;5;241m=\u001b[39mval_transform)\n",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36mImageDataset.__init__\u001b[0;34m(self, data_frame, img_dir, image_names, tile_size, stride, transform, keep_empty_prob)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_prob \u001b[38;5;241m=\u001b[39m keep_empty_prob\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiles_index \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_tile_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 56\u001b[0m, in \u001b[0;36mImageDataset._prepare_tile_indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m         mask[cloud_labels\u001b[38;5;241m.\u001b[39mindex(label)] \u001b[38;5;241m=\u001b[39m rle_to_array(rle)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# --- Pad to tile size ---\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m _, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_tile_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# --- Iterate over tiles ---\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y, x \u001b[38;5;129;01min\u001b[39;00m product(y_positions, x_positions):\n",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m, in \u001b[0;36mImageDataset.pad_to_tile_size\u001b[0;34m(self, image, mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m pad_w \u001b[38;5;241m=\u001b[39m (tw \u001b[38;5;241m-\u001b[39m w \u001b[38;5;241m%\u001b[39m tw) \u001b[38;5;241m%\u001b[39m tw\n\u001b[1;32m     30\u001b[0m image_padded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m     31\u001b[0m     image, ((\u001b[38;5;241m0\u001b[39m, pad_h), (\u001b[38;5;241m0\u001b[39m, pad_w), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m mask_padded \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_h\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_w\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_padded, mask_padded\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/numpy/lib/arraypad.py:808\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, width_pair, value_pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes, pad_width, values):\n\u001b[1;32m    807\u001b[0m         roi \u001b[38;5;241m=\u001b[39m _view_roi(padded, original_area_slice, axis)\n\u001b[0;32m--> 808\u001b[0m         _set_pad_area(roi, axis, width_pair, value_pair)\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Do nothing as _pad_simple already returned the correct result\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Datasets and DataLoader\n",
    "train_dataset = ImageDataset(df, train_dir, train_images, tile_size=tile_size, stride=stride, transform=train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                              pin_memory=True, persistent_workers = True, prefetch_factor = 5)\n",
    "\n",
    "test_dataset = ImageDataset(df, train_dir, test_images, tile_size=tile_size, stride=stride, transform=val_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                              pin_memory=True, persistent_workers = True, prefetch_factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pos weight for BCE\n",
    "# set num_workers=0 and disable persistent_workers and prefetch_factor before calculating\n",
    "if False:\n",
    "    pos_weight = np.zeros(4)\n",
    "    neg_pix = np.zeros(4)\n",
    "    pos_pix = np.zeros(4)\n",
    "\n",
    "    for image, mask in train_dataloader:\n",
    "        # Move batch tensors to device\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "        pos_pix += np.sum(mask==1, axis=(0,2,3))\n",
    "        neg_pix += np.sum(mask==0, axis=(0,2,3))\n",
    "\n",
    "    pos_weight = neg_pix / pos_pix\n",
    "    print(pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b4\",        # backbone\n",
    "    encoder_weights=\"imagenet\",            # use pretrained ImageNet weights\n",
    "    in_channels=3,                         # RGB images\n",
    "    classes=4,                             # 4 cloud types\n",
    "    decoder_use_batchnorm=True\n",
    ")\n",
    "\n",
    "# Inject dropout layers into decoder blocks\n",
    "for i, block in enumerate(model.decoder.blocks):\n",
    "    model.decoder.blocks[i].add_module(f\"dropout_{i}\", nn.Dropout2d(p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose model\n",
    "if model_name == \"custom\":\n",
    "    model = Network(num_filters).to(device)\n",
    "elif model_name == \"pretrained\":\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    print(\"Choose a valid Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d4fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0eb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations ImageNet stats\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def denormalize(img_tensor):\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    img = std * img + mean  # undo normalization\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "if run_test_cases:\n",
    "\n",
    "    for image, mask in train_dataloader:\n",
    "        # Move batch tensors to device\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "        # Loop over images in the batch\n",
    "        for img, msk in zip(image, mask):\n",
    "            # Convert tensor -> NumPy for display\n",
    "            \n",
    "            print(np.sum(img.cpu().numpy()))\n",
    "            if np.sum(img.cpu().numpy()) == 0:\n",
    "                print(\"Error\")\n",
    "\n",
    "            print(\"image\")\n",
    "            plt.figure()\n",
    "            plt.imshow(denormalize(img))\n",
    "            plt.show(block=False)   # prevents overwriting\n",
    "            plt.pause(0.1)          # forces GUI flush\n",
    "\n",
    "            for m in msk:\n",
    "                print(\"mask\")\n",
    "                plt.imshow(m.cpu().numpy(), cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49169cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m image, mask \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device), mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 15\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, mask, split)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/segmentation_models_pytorch/base/model.py:66\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mor\u001b[39;00m is_torch_compiling()\n\u001b[1;32m     63\u001b[0m ):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m---> 66\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(features)\n\u001b[1;32m     69\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/segmentation_models_pytorch/encoders/efficientnet.py:77\u001b[0m, in \u001b[0;36mEfficientNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocks):\n\u001b[1;32m     76\u001b[0m     drop_connect_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_connect_rate \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocks)\n\u001b[0;32m---> 77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_connect_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_indexes:\n\u001b[1;32m     80\u001b[0m         features\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/segmentation_models_pytorch/encoders/_efficientnet.py:188\u001b[0m, in \u001b[0;36mMBConvBlock.forward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    185\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(x_squeezed) \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Pointwise Convolution\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_project_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn2(x)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Skip connection and drop connect\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/segmentation_models_pytorch/encoders/_efficientnet.py:588\u001b[0m, in \u001b[0;36mConv2dStaticSamePadding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    587\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_padding(x)\n\u001b[0;32m--> 588\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Dynamic weigthing of Dice and BCE loss \n",
    "    split = 1.0 - (epoch / (num_epochs - 1)) * 0.5\n",
    "    \n",
    "    for image, mask in train_dataloader:\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, mask, split)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    dice = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, mask in test_dataloader:\n",
    "            image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, mask, split)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            #hard_preds = torch.where(preds > 0.5, 1.0, 0.0)\n",
    "            dice += dice_coef(preds, mask)\n",
    "        val_loss /= len(test_dataloader)\n",
    "        scheduler.step(val_loss)\n",
    "        dice /= len(test_dataloader)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Val loss: {val_loss:.4f}\")\n",
    "    print(f\"Dice coefficient: {dice:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "num_plots = 10\n",
    "count = 1\n",
    "\n",
    "save_img = 3\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, mask in test_dataloader:\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, mask, split)\n",
    "        \n",
    "        mask, preds = mask.cpu().numpy(), preds.cpu().numpy()\n",
    "\n",
    "        print(\"Image\")\n",
    "        plt.imshow(denormalize(image[0,:,:,:]))\n",
    "        if save_img == count:\n",
    "            plt.savefig(\"cloud_image.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Mask\")\n",
    "        plt.imshow(mask[0,0,:,:], vmax=1.0, vmin=0.0, cmap=\"Greys\")\n",
    "        if save_img == count:\n",
    "            plt.savefig(\"cloud_mask.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Prediction\")\n",
    "        plt.imshow(preds[0,0,:,:], vmax=1.0, vmin=0.0, cmap=\"Greys\")\n",
    "        if save_img == count:\n",
    "            plt.savefig(\"cloud_pred.png\")\n",
    "        plt.show()\n",
    "        print(\"##########################################\")\n",
    "\n",
    "        count += 1\n",
    "        if count == num_plots:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nest_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

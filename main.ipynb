{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e0472-1f98-4a5b-950b-5ab84f66411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f37bbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_cases = False # Set to True if debugging is required\n",
    "cloud_labels = [\"Flower\", \"Gravel\", \"Fish\", \"Sugar\"] # All possible labels for the clouds\n",
    "\n",
    "# Original Image resolutions\n",
    "in_res_y = 1400\n",
    "in_res_x = 2100\n",
    "\n",
    "# New Image resolutions\n",
    "new_res_y = 512\n",
    "new_res_x = 768\n",
    "\n",
    "# data directories\n",
    "test_dir = \"./test_images\"\n",
    "train_dir = \"./train_images\"\n",
    "\n",
    "# Training params\n",
    "num_filters = 32 # Number of filters in first conv layer  \n",
    "num_train_images = 1024\n",
    "num_test_images = 256\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 40\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f78d74dc-82d1-40ff-a5b5-a9369c14a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe\n",
    "df = pd.read_csv('train.csv')\n",
    "df[['Image', 'Label']] = df['Image_Label'].str.split('_', expand=True)\n",
    "\n",
    "# Find images with at least one mask\n",
    "df_nonempty = df.groupby(\"Image\")[\"EncodedPixels\"].apply(lambda x: x.notna().any()).reset_index()\n",
    "df_nonempty = df_nonempty[df_nonempty[\"EncodedPixels\"] == True]\n",
    "valid_images = df_nonempty[\"Image\"].unique().tolist()\n",
    "\n",
    "image_names = [img for img in sorted(os.listdir(train_dir)) if img in valid_images]\n",
    "\n",
    "# remove broken images\n",
    "def find_broken_images(img_dir):\n",
    "\n",
    "    # Load possibly saved broken file\n",
    "    if os.path.exists(f\"{img_dir}_broken.pkl\"):\n",
    "        with open(f\"{img_dir}_broken.pkl\", \"rb\") as f:\n",
    "            broken = pickle.load(f)\n",
    "        return broken\n",
    "     \n",
    "    broken = []\n",
    "    for f in os.listdir(img_dir):\n",
    "        path = os.path.join(img_dir, f)\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            broken.append(f)\n",
    "\n",
    "    with open(f\"{img_dir}_broken.pkl\", \"wb\") as f:\n",
    "        pickle.dump(broken, f)\n",
    "\n",
    "    return broken\n",
    "\n",
    "#broken_train = find_broken_images(\"./train_images\")\n",
    "#broken_test = find_broken_images(\"./test_images\")\n",
    "\n",
    "#print(\"Broken train images:\", broken_train)\n",
    "#print(\"Broken test images:\", broken_test)\n",
    "\n",
    "# Remove them from your image list\n",
    "#image_names = [f for f in image_names if f not in broken_train]\n",
    "\n",
    "train_images = image_names[:num_train_images]\n",
    "test_images = image_names[num_train_images:num_train_images+num_test_images]\n",
    "\n",
    "if run_test_cases:\n",
    "    print(df[['Image', 'Label', 'EncodedPixels']].head(8))\n",
    "    print()\n",
    "    print(df['Image'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b4364d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels and rle from image name\n",
    "def get_labels_rle(image_name: str, df) -> list:\n",
    "    rles = df[df['Image'] == image_name]['EncodedPixels'].to_list()\n",
    "    labels = df[df['Image'] == image_name]['Label'].to_list()\n",
    "    return rles, labels\n",
    "\n",
    "# Debugging\n",
    "if run_test_cases:\n",
    "\n",
    "    # Get Files\n",
    "    test_train_images = os.listdir(train_dir)[:4]\n",
    "    print(f\"Train images: {test_train_images}\")\n",
    "\n",
    "    for image in test_train_images:\n",
    "        rles, labels = get_labels_rle(f\"{image}\", df)\n",
    "        for rle, label in zip(rles, labels):\n",
    "            print(f\"Label: {label} \\n rle: {rle} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "095e0668-beaa-4f16-86c3-2d357ca784a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rle mask encoding into 2D arrays\n",
    "def rle_to_array(rle_list: list) -> np.array:\n",
    "\n",
    "    # Create empty array for\n",
    "    array = np.zeros(in_res_y * in_res_x)\n",
    "\n",
    "    # Skip if cloud formation is not on picture\n",
    "    if not rle_list or pd.isna(rle_list):\n",
    "        mask = array.reshape((in_res_x, in_res_y), order=\"A\").T\n",
    "        return mask\n",
    "    \n",
    "    rle_array = np.array(list(map(int, rle_list.split())), dtype=int)\n",
    "    start_pixels = rle_array[::2] - 1 # Offset because pixel 1 is arr position 0\n",
    "    num_pixels = rle_array[1::2]\n",
    "\n",
    "    # Create 2D mask\n",
    "    for start_pixel, num_pixels in zip(start_pixels, num_pixels): # Format is [start_idx_0, num_pixels_0 ...]\n",
    "        array[start_pixel:start_pixel+num_pixels] = 1.0\n",
    "    \n",
    "    # Reshape\n",
    "    mask = array.reshape((in_res_x, in_res_y), order=\"A\").T # 2D array of [Height, Width]\n",
    "\n",
    "    return mask\n",
    "\n",
    "# For debugging\n",
    "if run_test_cases:\n",
    "\n",
    "    # Get Files\n",
    "    test_train_images = os.listdir(train_dir)[:2]\n",
    "    print(f\"Train images: {test_train_images}\")\n",
    "\n",
    "    # Plot files\n",
    "    for image_name in test_train_images:\n",
    "        img = cv2.imread(f\"{train_dir}/{image_name}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (new_res_x, new_res_y), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        rles, labels = get_labels_rle(image_name, df)\n",
    "\n",
    "        # Raw Image\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        for rle, label in zip(rles, labels):\n",
    "\n",
    "            # Masked Image\n",
    "            mask = rle_to_array(rle)\n",
    "            mask = cv2.resize(mask, (new_res_x, new_res_y), interpolation=cv2.INTER_NEAREST)\n",
    "            print(np.unique(mask))\n",
    "            print(f\"titel: {label}\")\n",
    "            plt.imshow(mask, cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "            #plt.imshow(mask[:, :, None].repeat(3, axis=-1)*img, cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "            plt.show()\n",
    "        print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b649f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(preds, target, eps=1e-6):\n",
    "    # [B, 4, H, W]\n",
    "\n",
    "    preds = torch.sigmoid(preds)\n",
    "    overlap = (preds * target).sum((2,3))\n",
    "\n",
    "    dice = (2. * overlap + eps) / (preds.sum((2,3)) + target.sum((2,3)) + eps)\n",
    "\n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e780c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(preds, target, eps=1e-6):\n",
    "    return 1 - dice_coef(preds, target, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4052a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([5.0]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "206aadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, target, eps=1e-6):\n",
    "    return 0.25 * dice_loss(preds, target, eps) + 0.75 * bce_loss(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f993a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose([\n",
    "    # --- geometric transforms ---\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.2),\n",
    "\n",
    "    # --- photometric transforms ---\n",
    "    #A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.3),\n",
    "    #A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=5, val_shift_limit=5, p=0.3),\n",
    "\n",
    "    # --- spatial transforms ---\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.02,\n",
    "        scale_limit=0.05,\n",
    "        rotate_limit=10,\n",
    "        border_mode=cv2.BORDER_REFLECT,\n",
    "        p=0.3\n",
    "    ),\n",
    "\n",
    "    # --- normalization ---\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    # --- ensure correct shape ---\n",
    "    A.Resize(height=new_res_y, width=new_res_x),\n",
    "\n",
    "    # --- to tensor ---\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    # --- normalization ---\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    # --- ensure correct shape ---\n",
    "    A.Resize(height=new_res_y, width=new_res_x),\n",
    "\n",
    "    # --- to tensor ---\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c257c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_frame, img_dir, image_names, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.image_names = image_names\n",
    "        self.data_frame = data_frame\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        image_name = self.image_names[idx]\n",
    "\n",
    "        # --- Load image ---\n",
    "        image = cv2.imread(f\"{self.img_dir}/{image_name}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # --- Downsample image ---\n",
    "        image = cv2.resize(image, (new_res_x, new_res_y), interpolation=cv2.INTER_AREA)\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        rles, _ = get_labels_rle(image_name, self.data_frame)\n",
    "\n",
    "        # --- Create mask ---\n",
    "        mask = np.zeros((4, in_res_y, in_res_x), dtype=np.float32)\n",
    "        for i, rle in enumerate(rles):\n",
    "            single_mask = rle_to_array(rle)\n",
    "            mask[i, :, :] = single_mask\n",
    "\n",
    "        # --- Downsample mask per channel ---\n",
    "        mask_resized = np.zeros((4, new_res_y, new_res_x), dtype=np.float32)\n",
    "        for i in range(4):\n",
    "            mask_resized[i, :, :] = cv2.resize(\n",
    "                    mask[i],\n",
    "                    (new_res_x, new_res_y),\n",
    "                    interpolation=cv2.INTER_NEAREST\n",
    "                )\n",
    "        mask = mask_resized\n",
    "\n",
    "        # --- Transformations ---\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                mask=mask.transpose(1, 2, 0)\n",
    "            )\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"].permute(2, 0, 1)  # [C, H, W]\n",
    "        else:\n",
    "            # manually convert only if no transform applied\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "            mask = torch.from_numpy(mask).float()\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13727c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size, padding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, conv_kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, conv_kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv_block(x)\n",
    "        return output # [batch, out_channels, H_out, W_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7b7b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolBlock(nn.Module):\n",
    "    def __init__(self, downsample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(downsample)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pool(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a4cf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, channels, upsample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up_sample_block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=upsample, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(channels, channels // upsample, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        upsample_block = self.up_sample_block(x)\n",
    "        return upsample_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5351dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "\n",
    "        # in_channels, out_channels, conv_kernel_size, padding, pool_kernel_size\n",
    "        self.encoder_layer_0 = ConvolutionBlock(3, filters, 3, 1)\n",
    "        self.encoder_layer_1 = ConvolutionBlock(filters, 2*filters, 3, 1)\n",
    "        self.encoder_layer_2 = ConvolutionBlock(2*filters, 4*filters, 3, 1)\n",
    "\n",
    "        self.pool_block_0 = PoolBlock(downsample=2)\n",
    "        self.pool_block_1 = PoolBlock(downsample=2)\n",
    "        self.pool_block_2 = PoolBlock(downsample=2)\n",
    "\n",
    "        self.bottle_neck = nn.Conv2d(4*filters, 8*filters, 3, padding=1)\n",
    "\n",
    "        # in_channels, out_channels, conv_kernel_size, stride\n",
    "        self.decoder_layer_2 = ConvolutionBlock(8*filters, 4*filters, 3, 1)\n",
    "        self.decoder_layer_1 = ConvolutionBlock(4*filters, 2*filters, 3, 1)\n",
    "        self.decoder_layer_0 = ConvolutionBlock(2*filters, filters, 3, 1)\n",
    "\n",
    "        self.up_sample_block2 = UpSampleBlock(8 * filters, upsample=2)\n",
    "        self.up_sample_block1 = UpSampleBlock(4 * filters, upsample=2)\n",
    "        self.up_sample_block0 = UpSampleBlock(2 * filters, upsample=2)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(filters, 4, 1)\n",
    "\n",
    "    def forward(self, x): # [Batch, Color, Height, Width]\n",
    "        \n",
    "        enc0 = self.encoder_layer_0(x) # [B, num_filters, H, W]\n",
    "        pool0 = self.pool_block_0(enc0) # [B, num_filters, H / 2, W / 2]\n",
    "\n",
    "        enc1 = self.encoder_layer_1(pool0) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "        pool1 = self.pool_block_1(enc1) # [B, 2 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        enc2 = self.encoder_layer_2(pool1) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "        pool2 = self.pool_block_2(enc2) # [B, 4 * num_filters, H / 8, W / 8]\n",
    "\n",
    "        bottle_neck = self.bottle_neck(pool2) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        up2 = self.up_sample_block2(bottle_neck) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "        concat2 = torch.cat([up2, enc2], dim=1) # [B, 8 * num_filters, H / 4, W / 4]\n",
    "        dec2 = self.decoder_layer_2(concat2) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        up1 = self.up_sample_block1(dec2) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "        concat1 = torch.cat([up1, enc1], dim=1) # [B, 4 * num_filters, H / 2, W / 2]\n",
    "        dec1 = self.decoder_layer_1(concat1) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "\n",
    "        up0 = self.up_sample_block0(dec1) # [B, num_filters, H, W]\n",
    "        concat0 = torch.cat([up0, enc0], dim=1) # [B, 2 * num_filters, H, W]\n",
    "        dec0 = self.decoder_layer_0(concat0) # [B, num_filters, H, W]\n",
    "\n",
    "        logits = self.output_layer(dec0) # [B, 4, H, W]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75ef01b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and DataLoader\n",
    "train_dataset = ImageDataset(df, train_dir, train_images, transform=train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
    "\n",
    "test_dataset = ImageDataset(df, train_dir, test_images, transform=val_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b4\",        # backbone\n",
    "    encoder_weights=\"imagenet\",            # use pretrained ImageNet weights\n",
    "    in_channels=3,                         # RGB images\n",
    "    classes=4,                             # 4 cloud types\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_dropout=0.3 \n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Network(num_filters).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a0eb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations ImageNet stats\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def denormalize(img_tensor):\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    img = std * img + mean  # undo normalization\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "if run_test_cases:\n",
    "\n",
    "    for image, mask in train_dataloader:\n",
    "        # Move batch tensors to device\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "        # Loop over images in the batch\n",
    "        for img, msk in zip(image, mask):\n",
    "            # Convert tensor -> NumPy for display\n",
    "            \n",
    "            print(np.sum(img.cpu().numpy()))\n",
    "            if np.sum(img.cpu().numpy()) == 0:\n",
    "                print(\"Error\")\n",
    "\n",
    "            print(\"image\")\n",
    "            plt.figure()\n",
    "            plt.imshow(denormalize(img))\n",
    "            plt.show(block=False)   # prevents overwriting\n",
    "            plt.pause(0.1)          # forces GUI flush\n",
    "\n",
    "            for m in msk:\n",
    "                print(\"mask\")\n",
    "                plt.imshow(m.cpu().numpy(), cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49169cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     20\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:378\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke ``unscale_(optimizer)`` followed by parameter update, if gradients are not infs/NaN.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    Closure use is not currently supported.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosure\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m     )\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/nest/nest_env/lib/python3.12/site-packages/torch/optim/adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    388\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for image, mask in train_dataloader:\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    dice = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, mask in test_dataloader:\n",
    "            image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, mask)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            hard_preds = torch.where(F.sigmoid(preds) > 0.5, 1.0, 0.0)\n",
    "            dice += dice_coef(hard_preds, mask)\n",
    "        val_loss /= len(test_dataloader)\n",
    "        scheduler.step(val_loss)\n",
    "        dice /= len(test_dataloader)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Val loss: {val_loss:.4f}\")\n",
    "    print(f\"Dice coefficient: {dice:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e252a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nest_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2e0472-1f98-4a5b-950b-5ab84f66411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37bbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c56a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_cases = False # Set to True if debugging is required\n",
    "cloud_labels = [\"Flower\", \"Gravel\", \"Fish\", \"Sugar\"] # All possible labels for the clouds\n",
    "\n",
    "# Original Image resolutions\n",
    "in_res_y = 1400\n",
    "in_res_x = 2100\n",
    "\n",
    "# New Image resolutions\n",
    "new_res_y = 512\n",
    "new_res_x = 768\n",
    "\n",
    "# data directories\n",
    "test_dir = \"./test_images\"\n",
    "train_dir = \"./train_images\"\n",
    "\n",
    "# Training params\n",
    "num_filters = 32 # Number of filters in first conv layer  \n",
    "num_train_images = 5040\n",
    "num_test_images = 256\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 60\n",
    "lr = 3e-4\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78d74dc-82d1-40ff-a5b5-a9369c14a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe\n",
    "df = pd.read_csv('train.csv')\n",
    "df[['Image', 'Label']] = df['Image_Label'].str.split('_', expand=True)\n",
    "\n",
    "# Find images with at least one mask\n",
    "df_nonempty = df.groupby(\"Image\")[\"EncodedPixels\"].apply(lambda x: x.notna().any()).reset_index()\n",
    "df_nonempty = df_nonempty[df_nonempty[\"EncodedPixels\"] == True]\n",
    "valid_images = df_nonempty[\"Image\"].unique().tolist()\n",
    "\n",
    "image_names = [img for img in sorted(os.listdir(train_dir)) if img in valid_images]\n",
    "\n",
    "# remove broken images\n",
    "def find_broken_images(img_dir):\n",
    "\n",
    "    # Load possibly saved broken file\n",
    "    if os.path.exists(f\"{img_dir}_broken.pkl\"):\n",
    "        with open(f\"{img_dir}_broken.pkl\", \"rb\") as f:\n",
    "            broken = pickle.load(f)\n",
    "        return broken\n",
    "     \n",
    "    broken = []\n",
    "    for f in os.listdir(img_dir):\n",
    "        path = os.path.join(img_dir, f)\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            broken.append(f)\n",
    "\n",
    "    with open(f\"{img_dir}_broken.pkl\", \"wb\") as f:\n",
    "        pickle.dump(broken, f)\n",
    "\n",
    "    return broken\n",
    "\n",
    "#broken_train = find_broken_images(\"./train_images\")\n",
    "#broken_test = find_broken_images(\"./test_images\")\n",
    "\n",
    "#print(\"Broken train images:\", broken_train)\n",
    "#print(\"Broken test images:\", broken_test)\n",
    "\n",
    "# Remove them from your image list\n",
    "#image_names = [f for f in image_names if f not in broken_train]\n",
    "\n",
    "train_images = image_names[:num_train_images]\n",
    "test_images = image_names[num_train_images:num_train_images+num_test_images]\n",
    "\n",
    "if run_test_cases:\n",
    "    print(df[['Image', 'Label', 'EncodedPixels']].head(8))\n",
    "    print()\n",
    "    print(df['Image'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4364d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels and rle from image name\n",
    "def get_labels_rle(image_name: str, df) -> list:\n",
    "    rles = df[df['Image'] == image_name]['EncodedPixels'].to_list()\n",
    "    labels = df[df['Image'] == image_name]['Label'].to_list()\n",
    "    return rles, labels\n",
    "\n",
    "# Debugging\n",
    "if run_test_cases:\n",
    "\n",
    "    # Get Files\n",
    "    test_train_images = os.listdir(train_dir)[:4]\n",
    "    print(f\"Train images: {test_train_images}\")\n",
    "\n",
    "    for image in test_train_images:\n",
    "        rles, labels = get_labels_rle(f\"{image}\", df)\n",
    "        for rle, label in zip(rles, labels):\n",
    "            print(f\"Label: {label} \\n rle: {rle} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095e0668-beaa-4f16-86c3-2d357ca784a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rle mask encoding into 2D arrays\n",
    "def rle_to_array(rle_list: list) -> np.array:\n",
    "\n",
    "    # Create empty array for\n",
    "    array = np.zeros(in_res_y * in_res_x)\n",
    "\n",
    "    # Skip if cloud formation is not on picture\n",
    "    if not rle_list or pd.isna(rle_list):\n",
    "        mask = array.reshape((in_res_x, in_res_y), order=\"A\").T\n",
    "        return mask\n",
    "    \n",
    "    rle_array = np.array(list(map(int, rle_list.split())), dtype=int)\n",
    "    start_pixels = rle_array[::2] - 1 # Offset because pixel 1 is arr position 0\n",
    "    num_pixels = rle_array[1::2]\n",
    "\n",
    "    # Create 2D mask\n",
    "    for start_pixel, num_pixels in zip(start_pixels, num_pixels): # Format is [start_idx_0, num_pixels_0 ...]\n",
    "        array[start_pixel:start_pixel+num_pixels] = 1.0\n",
    "    \n",
    "    # Reshape\n",
    "    mask = array.reshape((in_res_x, in_res_y), order=\"A\").T # 2D array of [Height, Width]\n",
    "\n",
    "    return mask\n",
    "\n",
    "# For debugging\n",
    "if run_test_cases:\n",
    "\n",
    "    # Get Files\n",
    "    test_train_images = os.listdir(train_dir)[:2]\n",
    "    print(f\"Train images: {test_train_images}\")\n",
    "\n",
    "    # Plot files\n",
    "    for image_name in test_train_images:\n",
    "        img = cv2.imread(f\"{train_dir}/{image_name}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (new_res_x, new_res_y), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        rles, labels = get_labels_rle(image_name, df)\n",
    "\n",
    "        # Raw Image\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        for rle, label in zip(rles, labels):\n",
    "\n",
    "            # Masked Image\n",
    "            mask = rle_to_array(rle)\n",
    "            mask = cv2.resize(mask, (new_res_x, new_res_y), interpolation=cv2.INTER_NEAREST)\n",
    "            print(np.unique(mask))\n",
    "            print(f\"titel: {label}\")\n",
    "            plt.imshow(mask, cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "            #plt.imshow(mask[:, :, None].repeat(3, axis=-1)*img, cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "            plt.show()\n",
    "        print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b649f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(preds, target, eps=1e-6):\n",
    "    # [B, 4, H, W]\n",
    "\n",
    "    preds = torch.sigmoid(preds)\n",
    "    overlap = (preds * target).sum((2,3))\n",
    "\n",
    "    dice = (2. * overlap + eps) / (preds.sum((2,3)) + target.sum((2,3)) + eps)\n",
    "\n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e780c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(preds, target, eps=1e-6):\n",
    "    return 1 - dice_coef(preds, target, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4052a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([5.0]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "206aadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, target, split=0.5, eps=1e-6):\n",
    "    return split * dice_loss(preds, target, eps) + (1 - split) * bce_loss(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/lhm9rkfd5mq5nqs778m806v40000gn/T/ipykernel_70940/112616903.py:12: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n",
      "  A.ElasticTransform(alpha=50, sigma=8, alpha_affine=8, p=0.3),\n",
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose([\n",
    "    # --- geometric transforms ---\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.2),\n",
    "\n",
    "    # --- photometric transforms ---\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.3),\n",
    "    A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=5, val_shift_limit=5, p=0.3),\n",
    "    \n",
    "    A.ElasticTransform(alpha=50, sigma=8, alpha_affine=8, p=0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "    A.GaussNoise(var_limit=(5, 20), p=0.3),\n",
    "\n",
    "    # --- spatial transforms ---\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.05,\n",
    "        scale_limit=0.1,\n",
    "        rotate_limit=15,\n",
    "        border_mode=cv2.BORDER_REFLECT,\n",
    "        p=0.5\n",
    "    ),\n",
    "\n",
    "    # --- normalization ---\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    # --- ensure correct shape ---\n",
    "    A.Resize(height=new_res_y, width=new_res_x),\n",
    "\n",
    "    # --- to tensor ---\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    # --- normalization ---\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    # --- ensure correct shape ---\n",
    "    A.Resize(height=new_res_y, width=new_res_x),\n",
    "\n",
    "    # --- to tensor ---\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c257c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_frame, img_dir, image_names, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.image_names = image_names\n",
    "        self.data_frame = data_frame\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        image_name = self.image_names[idx]\n",
    "\n",
    "        # --- Load image ---\n",
    "        image = cv2.imread(f\"{self.img_dir}/{image_name}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # --- Downsample image ---\n",
    "        image = cv2.resize(image, (new_res_x, new_res_y), interpolation=cv2.INTER_AREA)\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        rles, _ = get_labels_rle(image_name, self.data_frame)\n",
    "\n",
    "        # --- Create mask ---\n",
    "        mask = np.zeros((4, in_res_y, in_res_x), dtype=np.float32)\n",
    "        for i, rle in enumerate(rles):\n",
    "            single_mask = rle_to_array(rle)\n",
    "            mask[i, :, :] = single_mask\n",
    "\n",
    "        # --- Downsample mask per channel ---\n",
    "        mask_resized = np.zeros((4, new_res_y, new_res_x), dtype=np.float32)\n",
    "        for i in range(4):\n",
    "            mask_resized[i, :, :] = cv2.resize(\n",
    "                    mask[i],\n",
    "                    (new_res_x, new_res_y),\n",
    "                    interpolation=cv2.INTER_NEAREST\n",
    "                )\n",
    "        mask = mask_resized\n",
    "\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        # --- Transformations ---\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                mask=mask.transpose(1, 2, 0)\n",
    "            )\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"].permute(2, 0, 1)  # [C, H, W]\n",
    "        else:\n",
    "            # manually convert only if no transform applied\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "            mask = torch.from_numpy(mask).float()\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13727c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size, padding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, conv_kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, conv_kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv_block(x)\n",
    "        return output # [batch, out_channels, H_out, W_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7b7b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolBlock(nn.Module):\n",
    "    def __init__(self, downsample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(downsample)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pool(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a4cf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, channels, upsample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up_sample_block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=upsample, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(channels, channels // upsample, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        upsample_block = self.up_sample_block(x)\n",
    "        return upsample_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5351dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "\n",
    "        # in_channels, out_channels, conv_kernel_size, padding, pool_kernel_size\n",
    "        self.encoder_layer_0 = ConvolutionBlock(3, filters, 3, 1)\n",
    "        self.encoder_layer_1 = ConvolutionBlock(filters, 2*filters, 3, 1)\n",
    "        self.encoder_layer_2 = ConvolutionBlock(2*filters, 4*filters, 3, 1)\n",
    "\n",
    "        self.pool_block_0 = PoolBlock(downsample=2)\n",
    "        self.pool_block_1 = PoolBlock(downsample=2)\n",
    "        self.pool_block_2 = PoolBlock(downsample=2)\n",
    "\n",
    "        self.bottle_neck = nn.Conv2d(4*filters, 8*filters, 3, padding=1)\n",
    "\n",
    "        # in_channels, out_channels, conv_kernel_size, stride\n",
    "        self.decoder_layer_2 = ConvolutionBlock(8*filters, 4*filters, 3, 1)\n",
    "        self.decoder_layer_1 = ConvolutionBlock(4*filters, 2*filters, 3, 1)\n",
    "        self.decoder_layer_0 = ConvolutionBlock(2*filters, filters, 3, 1)\n",
    "\n",
    "        self.up_sample_block2 = UpSampleBlock(8 * filters, upsample=2)\n",
    "        self.up_sample_block1 = UpSampleBlock(4 * filters, upsample=2)\n",
    "        self.up_sample_block0 = UpSampleBlock(2 * filters, upsample=2)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(filters, 4, 1)\n",
    "\n",
    "    def forward(self, x): # [Batch, Color, Height, Width]\n",
    "        \n",
    "        enc0 = self.encoder_layer_0(x) # [B, num_filters, H, W]\n",
    "        pool0 = self.pool_block_0(enc0) # [B, num_filters, H / 2, W / 2]\n",
    "\n",
    "        enc1 = self.encoder_layer_1(pool0) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "        pool1 = self.pool_block_1(enc1) # [B, 2 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        enc2 = self.encoder_layer_2(pool1) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "        pool2 = self.pool_block_2(enc2) # [B, 4 * num_filters, H / 8, W / 8]\n",
    "\n",
    "        bottle_neck = self.bottle_neck(pool2) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        up2 = self.up_sample_block2(bottle_neck) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "        concat2 = torch.cat([up2, enc2], dim=1) # [B, 8 * num_filters, H / 4, W / 4]\n",
    "        dec2 = self.decoder_layer_2(concat2) # [B, 4 * num_filters, H / 4, W / 4]\n",
    "\n",
    "        up1 = self.up_sample_block1(dec2) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "        concat1 = torch.cat([up1, enc1], dim=1) # [B, 4 * num_filters, H / 2, W / 2]\n",
    "        dec1 = self.decoder_layer_1(concat1) # [B, 2 * num_filters, H / 2, W / 2]\n",
    "\n",
    "        up0 = self.up_sample_block0(dec1) # [B, num_filters, H, W]\n",
    "        concat0 = torch.cat([up0, enc0], dim=1) # [B, 2 * num_filters, H, W]\n",
    "        dec0 = self.decoder_layer_0(concat0) # [B, num_filters, H, W]\n",
    "\n",
    "        logits = self.output_layer(dec0) # [B, 4, H, W]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75ef01b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and DataLoader\n",
    "train_dataset = ImageDataset(df, train_dir, train_images, transform=train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
    "\n",
    "test_dataset = ImageDataset(df, train_dir, test_images, transform=val_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba05c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b4\",        # backbone\n",
    "    encoder_weights=\"imagenet\",            # use pretrained ImageNet weights\n",
    "    in_channels=3,                         # RGB images\n",
    "    classes=4,                             # 4 cloud types\n",
    "    decoder_use_batchnorm=True\n",
    ")\n",
    "\n",
    "# Inject dropout layers into decoder blocks\n",
    "for i, block in enumerate(model.decoder.blocks):\n",
    "    model.decoder.blocks[i].add_module(f\"dropout_{i}\", nn.Dropout2d(p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56f4849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Network(num_filters).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5989a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "573d4fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a0eb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations ImageNet stats\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def denormalize(img_tensor):\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    img = std * img + mean  # undo normalization\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "if run_test_cases:\n",
    "\n",
    "    for image, mask in train_dataloader:\n",
    "        # Move batch tensors to device\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "        # Loop over images in the batch\n",
    "        for img, msk in zip(image, mask):\n",
    "            # Convert tensor -> NumPy for display\n",
    "            \n",
    "            print(np.sum(img.cpu().numpy()))\n",
    "            if np.sum(img.cpu().numpy()) == 0:\n",
    "                print(\"Error\")\n",
    "\n",
    "            print(\"image\")\n",
    "            plt.figure()\n",
    "            plt.imshow(denormalize(img))\n",
    "            plt.show(block=False)   # prevents overwriting\n",
    "            plt.pause(0.1)          # forces GUI flush\n",
    "\n",
    "            for m in msk:\n",
    "                print(\"mask\")\n",
    "                plt.imshow(m.cpu().numpy(), cmap=\"grey\", vmin=0.0, vmax=1.0)\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49169cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 768, 3) uint8 253 0\n",
      "(4, 512, 768) float32\n",
      "\n",
      "(512, 768, 3) uint8 255 0\n",
      "(4, 512, 768) float32\n",
      "\n",
      "(512, 768, 3) uint8 254 0\n",
      "(4, 512, 768) float32\n",
      "\n",
      "(512, 768, 3) uint8 252 0\n",
      "(4, 512, 768) float32\n",
      "\n",
      "(512, 768, 3) uint8 254 0\n",
      "(4, 512, 768) float32\n",
      "\n",
      "(512, 768, 3) uint8 253 0\n",
      "(4, 512, 768) float32\n",
      "\n",
      "(512, 768, 3) uint8 250 0\n",
      "(4, 512, 768) float32\n",
      "\n",
      "(512, 768, 3) uint8 252 0\n",
      "(4, 512, 768) float32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/work/nest/nest_env/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Dynamic weigthing of Dice and BCE loss \n",
    "    split = 1 - (epoch / num_epochs) * 0.35\n",
    "    \n",
    "    for image, mask in train_dataloader:\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, mask, split)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    dice = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, mask in test_dataloader:\n",
    "            image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, mask, split)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            #hard_preds = torch.where(preds > 0.5, 1.0, 0.0)\n",
    "            dice += dice_coef(preds, mask)\n",
    "        val_loss /= len(test_dataloader)\n",
    "        scheduler.step(val_loss)\n",
    "        dice /= len(test_dataloader)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Val loss: {val_loss:.4f}\")\n",
    "    print(f\"Dice coefficient: {dice:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nest_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
